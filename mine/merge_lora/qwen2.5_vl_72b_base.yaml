### Note: DO NOT use quantized model or quantization_bit when merging lora adapters

### model
model_name_or_path: Qwen/Qwen2.5-VL-72B-Instruct
template: qwen2_vl
cache_dir: /workspace/modelscope
trust_remote_code: true

### export
export_dir: output/qwen2.5_vl_72b
export_size: 20 # 基于A100显存的分块优化
export_device: cpu  # cpu
export_legacy_format: false
